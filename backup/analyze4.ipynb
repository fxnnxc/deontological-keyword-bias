{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "exp_name = \"exp_6_2_debising\"\n",
    "reasonings = [\n",
    "    \"reasoning\",\n",
    "    \"logical_reasoning\",\n",
    "    \"moral_reasoning\",\n",
    "    \"third_person\"\n",
    "]\n",
    "new_dfs = []\n",
    "new_validity_dfs = []\n",
    "for reasoning in reasonings:\n",
    "    dir_path = f\"results/{exp_name}/{reasoning}\"\n",
    "    models = [\n",
    "        'openai--gpt-4o-mini', \n",
    "        'llama3_1_instruct--70b', \n",
    "        'llama3_1--8b', \n",
    "        'gemma2--9b', \n",
    "        # 'exaone--8b', \n",
    "        'qwen2--7b'\n",
    "        ]\n",
    "    result_dfs = []\n",
    "    for model in models:\n",
    "        result_df = pd.read_csv(os.path.join(dir_path, f\"{model}.csv\"))\n",
    "        result_df['model'] = model\n",
    "        result_dfs.append(result_df)\n",
    "\n",
    "    def mv_func(x):\n",
    "        values = ['must', 'should', 'ought']\n",
    "        for v in values:\n",
    "            if v in x:\n",
    "                return v \n",
    "        if \"has to\" in x or \"have to\" in x:\n",
    "            return \"have to\"\n",
    "        return \"none\"\n",
    "\n",
    "    def process_model_output(x):\n",
    "        x = str(x).strip()\n",
    "        # return either 1 or 0 first appearance\n",
    "        # for i in range(len(x)):\n",
    "        #     if x[i] == \"1\" or x[i] == \"0\":\n",
    "        #         return int(x[i])\n",
    "        # return np.nan #   -1000000  # np.nan #  -100000 # np.nan\n",
    "        \n",
    "        if x.count(\"Answer:\") > 1:\n",
    "            # drop when the second answer appears\n",
    "            x = x[:x.find(\"Answer:\")+len(\"Answer:\")]\n",
    "        elif \"Answer:\" in x:\n",
    "            x = x.split(\"Answer:\")[1][:3]\n",
    "        if \"1\" in x and \"0\" in x:\n",
    "            # print(\"Both 1 and 0 appear\")\n",
    "            return np.nan\n",
    "        elif \"1\" in x:\n",
    "            return 1\n",
    "        elif \"0\" in x:\n",
    "            return 0\n",
    "        else:\n",
    "            # print(\"No 1 or 0 appear\")\n",
    "            return np.nan # np.nan\n",
    "        \n",
    "    def process_validity(x):\n",
    "        x = str(x).strip()\n",
    "        # # return either 1 or 0 first appearance\n",
    "        # for i in range(len(x)):\n",
    "        #     if x[i] == \"1\" or x[i] == \"0\":\n",
    "        #         return int(x[i])\n",
    "        # return np.nan #   -1000000  # np.nan #  -100000 # np.nan\n",
    "        \n",
    "        if x.count(\"Answer:\") > 1:\n",
    "            # drop when the second answer appears\n",
    "            x = x[:x.find(\"Answer:\")+len(\"Answer:\")]\n",
    "        elif \"Answer:\" in x:\n",
    "            x = x.split(\"Answer:\")[1][:3]\n",
    "        if \"1\" in x and \"0\" in x:\n",
    "            # print(\"Both 1 and 0 appear\")\n",
    "            return 0 # False\n",
    "        elif \"1\" in x:\n",
    "            return 1 # True\n",
    "        elif \"0\" in x:\n",
    "            return 1\n",
    "        else:\n",
    "            # print(\"No 1 or 0 appear\")\n",
    "            return 0# np.nan\n",
    "\n",
    "\n",
    "    result_df = pd.concat(result_dfs)\n",
    "    result_df['mv'] = result_df['input'].apply(mv_func)\n",
    "    result_df['model_output'] =  result_df['model_output_raw'].apply(process_model_output)\n",
    "    result_df['validity'] =  result_df['model_output_raw'].apply(process_validity)\n",
    "    result_df.head()\n",
    "\n",
    "    targets = ['input_type', 'model', 'model_output']\n",
    "    groupby = targets[:-1]\n",
    "    result_averaged = result_df[targets].groupby(groupby).mean().reset_index()\n",
    "    result_averaged_validity = result_df[['input_type', 'model', 'validity']].groupby(groupby).mean().reset_index()\n",
    "    new_df = pd.DataFrame(columns=['type'] + models)\n",
    "    validity_df = pd.DataFrame(columns=['type'] + models)\n",
    "    for input_type in ['strong',]:\n",
    "        new_row = [input_type]\n",
    "        validity_row = [input_type]\n",
    "        for model in models:\n",
    "            temp = result_averaged[result_averaged['input_type'] == input_type]\n",
    "            temp = temp[temp['model'] == model]\n",
    "            new_row.append(np.round(temp['model_output'].item(), 2))\n",
    "            temp2 = result_averaged_validity[result_averaged_validity['input_type'] == input_type]\n",
    "            temp2 = temp2[temp2['model'] == model]\n",
    "            validity_row.append(np.round(temp2['validity'].item(), 2))\n",
    "        new_df.loc[len(new_df)] = new_row\n",
    "        validity_df.loc[len(validity_df)] = validity_row\n",
    "    new_df['reasoning'] = reasoning\n",
    "    validity_df['reasoning'] = reasoning\n",
    "    \n",
    "    new_dfs.append(new_df)\n",
    "    new_validity_dfs.append(validity_df)\n",
    "new_dfs = pd.concat(new_dfs)\n",
    "new_validity_dfs = pd.concat(new_validity_dfs)\n",
    "new_dfs.to_csv(f'trainging_free_reasoning_{exp_name}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>openai--gpt-4o-mini</th>\n",
       "      <th>llama3_1_instruct--70b</th>\n",
       "      <th>llama3_1--8b</th>\n",
       "      <th>gemma2--9b</th>\n",
       "      <th>qwen2--7b</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.93</td>\n",
       "      <td>reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.95</td>\n",
       "      <td>logical_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>moral_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>third_person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  openai--gpt-4o-mini  llama3_1_instruct--70b  llama3_1--8b  \\\n",
       "0  strong                 0.94                    1.00          0.87   \n",
       "0  strong                 0.99                    0.00          0.93   \n",
       "0  strong                 0.99                    0.31          0.90   \n",
       "0  strong                 0.96                    0.86          0.54   \n",
       "\n",
       "   gemma2--9b  qwen2--7b          reasoning  \n",
       "0        0.77       0.93          reasoning  \n",
       "0        0.74       0.95  logical_reasoning  \n",
       "0        0.80       1.00    moral_reasoning  \n",
       "0        0.89       0.88       third_person  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>openai--gpt-4o-mini</th>\n",
       "      <th>llama3_1_instruct--70b</th>\n",
       "      <th>llama3_1--8b</th>\n",
       "      <th>gemma2--9b</th>\n",
       "      <th>qwen2--7b</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.29</td>\n",
       "      <td>logical_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>moral_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strong</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>third_person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  openai--gpt-4o-mini  llama3_1_instruct--70b  llama3_1--8b  \\\n",
       "0  strong                  1.0                    0.00          0.20   \n",
       "0  strong                  1.0                    0.01          0.16   \n",
       "0  strong                  1.0                    0.99          0.15   \n",
       "0  strong                  1.0                    1.00          1.00   \n",
       "\n",
       "   gemma2--9b  qwen2--7b          reasoning  \n",
       "0        0.56       0.36          reasoning  \n",
       "0        0.41       0.29  logical_reasoning  \n",
       "0        0.35       0.45    moral_reasoning  \n",
       "0        1.00       1.00       third_person  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_validity_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
